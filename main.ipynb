{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from embedding import OneHotEmbedder, Doc2VecEmbedder\n",
    "from vectorstore import SimpleVectorDatabase\n",
    "from models import TextDoc, Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Document Similarity\n",
    "Create a python program that will compute the text document similarity between different documents. Your implementation will take a list of documents as an input text corpus, and it will compute a dictionary of words for the given corpus. Later, when a new document (i.e, search document) is provided, your implementation should provide a list of documents that are similar to the given search document, in descending order of their similarity with the search document.\n",
    "\n",
    "For computing similarity between any two documents in our question, you can use the following distance measures (optionally, you can also use any other measure as well).\n",
    "1. dot product between the two vectors\n",
    "2. distance norm (or Euclidean distance) between two vectors e.g. $|| u − v ||$\n",
    "\n",
    "As part of answering the question, you can also compare and comment on which of the two methods (or any other measure if you have used some other measure) will perform better and what are the reasons for it.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook guids trough the implementation and intended use of it. For additional details see the project report (pdf).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31945</td>\n",
       "      <td>https://en.wikipedia.org/wiki/List%20of%20metr...</td>\n",
       "      <td>List of metro systems</td>\n",
       "      <td>This list of metro systems includes electrifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37600</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Axis</td>\n",
       "      <td>Axis</td>\n",
       "      <td>Axis may refer to:\\n\\nMathematics\\nAxis (mathe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37945</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Giovanni%20Paisi...</td>\n",
       "      <td>Giovanni Paisiello</td>\n",
       "      <td>Giovanni Paisiello (or Paesiello; 9 May 1740 –...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20807</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Merlin</td>\n",
       "      <td>Merlin</td>\n",
       "      <td>Merlin (, , ) is a mythical figure prominently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>621</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Amphibian</td>\n",
       "      <td>Amphibian</td>\n",
       "      <td>Amphibians are ectothermic, tetrapod vertebrat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                                url  \\\n",
       "0  31945  https://en.wikipedia.org/wiki/List%20of%20metr...   \n",
       "1  37600                 https://en.wikipedia.org/wiki/Axis   \n",
       "2  37945  https://en.wikipedia.org/wiki/Giovanni%20Paisi...   \n",
       "3  20807               https://en.wikipedia.org/wiki/Merlin   \n",
       "4    621            https://en.wikipedia.org/wiki/Amphibian   \n",
       "\n",
       "                   title                                               text  \n",
       "0  List of metro systems  This list of metro systems includes electrifie...  \n",
       "1                   Axis  Axis may refer to:\\n\\nMathematics\\nAxis (mathe...  \n",
       "2     Giovanni Paisiello  Giovanni Paisiello (or Paesiello; 9 May 1740 –...  \n",
       "3                 Merlin  Merlin (, , ) is a mythical figure prominently...  \n",
       "4              Amphibian  Amphibians are ectothermic, tetrapod vertebrat...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets load the dataset. We use a small sample dataset from wikipedia here. But in the \n",
    "# `data_load` notebook, you can find a script to download a larger dataset. The larger\n",
    "# dataset will give better results, but is more computationally expensive. To test this\n",
    "# script, the small dataset is sufficient.\n",
    "\n",
    "# The dataset consists of multiple wikipedia articles. Each row contains the title of the\n",
    "# article, the text of the article as well as the url.\n",
    "dataset = pd.read_csv(\"data/wiki_subset_mini.csv\", sep=\",\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some random articles for later testing\n",
    "# draw random sample of 5 articles\n",
    "random.seed(42)\n",
    "testing_indices = random.sample(range(len(dataset)), 5)\n",
    "\n",
    "test_texts = dataset.loc[testing_indices,:].reset_index(drop=True)\n",
    "train_texts = dataset.loc[~dataset.index.isin(testing_indices),:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This parameter is used to set the vector dimension of the embedding algorithm and the \n",
    "# vector database. The higher the dimension, the more accurate the results will be, but\n",
    "# the more computationally expensive the calculations will be. OpenAI's embedding models\n",
    "# use a dimension of 1536 as a reference.\n",
    "# We suggest to use a dimension of 200 for the smaller datasets. For the larger datasets,\n",
    "# this parameter can be increased. The larger the vector dimension, the more accurate the\n",
    "# the embeddings are, because more information can be embedded. However, this might also\n",
    "# increase noise and is more computationally expensive.\n",
    "\n",
    "VECTOR_DIM = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 15:51:00,036::onehotembedder.py[_fit()]::INFO::Fitting embedder. Preprocessing documents...\n",
      "100%|██████████| 295/295 [00:13<00:00, 21.58it/s]\n",
      "2024-01-11 15:51:13,828::onehotembedder.py[_fit()]::INFO::Corpus created with 65281 words.\n",
      "2024-01-11 15:51:13,830::onehotembedder.py[_fit()]::INFO::Reducing vector dimensions: fitting PCA (n=150)\n"
     ]
    }
   ],
   "source": [
    "# In this step the embedding model is fitted. This means that the model is trained on the\n",
    "# given dataset. The user can choose which embedding algorithm to use - this implementation\n",
    "# is designed to be easily extendable to other embedding algorithms while keeping the user\n",
    "# interface the same. Every embedding implementation needs to have the methods `fit` and\n",
    "# `embed`. The `fit` method is used to train the model on the given dataset. The `embed`\n",
    "# method is used to embed a given text into a vector.\n",
    "\n",
    "# The `OneHotEmbedder` is a simple embedding algorithm that uses a one-hot encoding to create\n",
    "# the embedding-vectors. It is trained on the dataset by creating a vocabulary of all words\n",
    "# and then creating a very sparse vector for each text. Each vector signals which words are\n",
    "# present in the text. There are two different embedding methods available: additive and\n",
    "# ont-hot. The additive method is the default method, it counts the number of occurences of\n",
    "# each word in the text. The one-hot method only signals if a word is present in the text or\n",
    "# not, without counting. After creating the vector for one text, the vector has a length\n",
    "# of the corpus, which is really huge. To reduce the dimensionality of the vector, PCA is\n",
    "# applied to the vector. This reduces the dimensionality to the given `vector_dim`\n",
    "# parameter.\n",
    "\n",
    "# The `Doc2VecEmbedder` is a more complex embedding algorithm from the gensim library. It\n",
    "# is more computationally expensive, but also more accurate. Details can be found in the \n",
    "# gensim documentation. We won't go into detail here, since this is out of scope of this\n",
    "# excercise. (gensim documentation: https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "\n",
    "\n",
    "# Choose the embedder model to use: OneHot or Doc2Vec currently available\n",
    "embedder = OneHotEmbedder(vector_dim=VECTOR_DIM, embedding_method=\"additive\")\n",
    "#embedder = Doc2VecEmbedder(vector_dim=VECTOR_DIM)\n",
    "\n",
    "# fit the embedder model: this could take a view minutes\n",
    "embedder.fit(list(train_texts.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 295/295 [00:16<00:00, 18.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# In the next step we create a vector database. This database is used to store the texts\n",
    "# we want to search in later. We fill the database with the texts, embedded as vectors.\n",
    "# Again we designed this implementation to be easily extendable to other database\n",
    "# implementations. The only requirement is that the database has specific methods like\n",
    "# `sim_search` or `upsert` that takes a vector as an argument and updates or inserts a\n",
    "# vector into the database. \n",
    "\n",
    "# The `SimpleVectorDatabase` is a simple implementation of a vector database. It stores\n",
    "# the vectors in a simple dictionary. For each wikipedia article, we create a TexDoc from\n",
    "# the text. This TextDoc is the embedded as a vector using the embedder model, fitted \n",
    "# earlier. We add some metadata like the actual text, the title and the source and finally\n",
    "# upsert the vector into the database. This process might take some time, depending on the\n",
    "# size of the dataset.\n",
    "\n",
    "\n",
    "# create vector database instance\n",
    "vecdb = SimpleVectorDatabase(vector_dim=VECTOR_DIM)\n",
    "\n",
    "# create vectors and store them in the database\n",
    "for i in tqdm(range(len(train_texts))):\n",
    "    row = train_texts.iloc[i]\n",
    "    doc = TextDoc(row.text)\n",
    "    vec = Vector(\n",
    "        embedding=embedder.embed(doc),\n",
    "        data=TextDoc(doc),\n",
    "        metadata={\"type\": \"wikipedia\", \"src\": row.url, \"title\": row.title})\n",
    "    vecdb.upsert(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextDoc(36454): Stephen Gary Wozniak (; born August 11, 1950), also kno ...\n"
     ]
    }
   ],
   "source": [
    "# Now we fitted an embedding model and created a vector database. We can now try to find\n",
    "# similar documents in the database. First we need to embedd the new text into a vector\n",
    "# using the same embedding model.\n",
    "\n",
    "\n",
    "# Let's take the last row of the dataset and query the database for similar vectors.\n",
    "row = test_texts.iloc[3,:]\n",
    "new_doc = TextDoc(row.text)\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vector 0c333e22-49f0-42b6-b576-0fc1bc421361 (150) : TextDoc(36454): Stephen Gary Wozniak (; born August 11, 1950), also kno ...>\n"
     ]
    }
   ],
   "source": [
    "# Now we create a embedding vector from the TextDoc\n",
    "new_vec = Vector(\n",
    "    embedding=embedder.embed(new_doc),\n",
    "    data=TextDoc(new_doc),\n",
    "    metadata={\"type\": \"wikipedia\", \"src\": row.url, \"title\": row.title}\n",
    ")\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vector': <Vector d78f6097-7fe4-4285-beb4-a70771ae34e8 (150) : TextDoc(62194): Dolly Rebecca Parton (born January 19, 1946) is an Amer ...>,\n",
       "  'score': 4111.125644690877},\n",
       " {'vector': <Vector 35293ef6-b248-4bc5-9644-010a91562f4e (150) : TextDoc(29752): A wearable computer, also known as a wearable or body-b ...>,\n",
       "  'score': 4089.148729202497}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we can query the database for similar vectors. Select the measure to use and the\n",
    "# number of similar vectors to return. Again, this implementation is designed to be easily\n",
    "# extendable to other similarity measures. Currently implemented measures are: cosine,\n",
    "# euclidean and dot. \n",
    "# Each result is the vector found in the database and the corresponding similarity score.\n",
    "\n",
    "similar_vectors = vecdb.sim_search(new_vec, measure=\"dot\", k=2)\n",
    "similar_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector:       [title: Steve Wozniak] TextDoc(36454): Stephen Gary Wozniak (; born August 11, 1950), also kno ...\n",
      "Most similar: [title: Dolly Parton] TextDoc(62194): Dolly Rebecca Parton (born January 19, 1946) is an Amer ...\n",
      "Most similar: [title: Wearable computer] TextDoc(29752): A wearable computer, also known as a wearable or body-b ...\n"
     ]
    }
   ],
   "source": [
    "# Lets look at the results. Our search vector is the wikipedia article for Stephen Gary\n",
    "# Wozniak, the co-founder of Apple. The most similar vector in our database is the\n",
    "# wikipedia article of Dolly Parton, the american singer. The second most similar vector\n",
    "# is the article about a warable computer. This is a good result, since the article about\n",
    "# Dolly Parton is also about a famous person and the article about the warable computer is\n",
    "# also about a technical topic. The similarity measure is not perfect, but it is a good\n",
    "# start. The results can be improved by using a larger dataset and a more complex embedding.\n",
    "\n",
    "\n",
    "print(f\"Vector:       [title: {new_vec.metadata['title']}] {new_vec.data}\")\n",
    "for i in range(len(similar_vectors)):\n",
    "    print(f\"Most similar: [title: {similar_vectors[i]['vector'].metadata['title']}] {similar_vectors[i]['vector'].data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector:       [title: Steve Wozniak] TextDoc(36454): Stephen Gary Wozniak (; born August 11, 1950), also kno ...\n",
      "Most similar: [title: Wearable computer] TextDoc(29752): A wearable computer, also known as a wearable or body-b ...\n",
      "Most similar: [title: Activision] TextDoc(31711): Activision Publishing, Inc. is an American video game p ...\n"
     ]
    }
   ],
   "source": [
    "# Lets try using another measure: the cosine similarity. This measure is more robust to\n",
    "# different vector lengths and is a good choice for text embeddings. The results are\n",
    "# even better than before. We once again get the article about warable computers, but also\n",
    "# the article about Activision, a video game company. This is a good result.\n",
    "\n",
    "\n",
    "similar_vectors = vecdb.sim_search(new_vec, measure=\"cosine\", k=2)\n",
    "\n",
    "print(f\"Vector:       [title: {new_vec.metadata['title']}] {new_vec.data}\")\n",
    "for i in range(len(similar_vectors)):\n",
    "    print(f\"Most similar: [title: {similar_vectors[i]['vector'].metadata['title']}] {similar_vectors[i]['vector'].data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector:       [title: Steve Wozniak] TextDoc(36454): Stephen Gary Wozniak (; born August 11, 1950), also kno ...\n",
      "Most similar: [title: Akio Morita] TextDoc(7900): Akio Morita (January 26, 1921 – October 3, 1999) was a  ...\n",
      "Most similar: [title: Microserfs] TextDoc(10410): Microserfs, published by HarperCollins in 1995, is an e ...\n"
     ]
    }
   ],
   "source": [
    "# Lastly lets try the euclidean distance. This measure returns an article about Akito\n",
    "# Morita, the co-founder of Sony. The second most similar article is about Microserfs, a\n",
    "# novel about a group of computer programmers. This also seems to be a pretty good result.\n",
    "\n",
    "\n",
    "similar_vectors = vecdb.sim_search(new_vec, measure=\"euclidean\", k=2)\n",
    "\n",
    "print(f\"Vector:       [title: {new_vec.metadata['title']}] {new_vec.data}\")\n",
    "for i in range(len(similar_vectors)):\n",
    "    print(f\"Most similar: [title: {similar_vectors[i]['vector'].metadata['title']}] {similar_vectors[i]['vector'].data}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
